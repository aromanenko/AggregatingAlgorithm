{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba5efd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.forecasting.theta import ThetaModel\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.special import rel_entr, kl_div\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# plt.style.use('seaborn')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dc2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density_forecast(ts, horizon, base_alg, base_params={}, fit_params={}, exog=None,\n",
    "                         bins='auto', omega=None, fittedvalues=False):\n",
    "    \"\"\"\n",
    "    Returns a list of density dictionaries {'bins': np.array, 'probs': np.array, 'dotted_forecast': float}.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : array_like\n",
    "        The time series to model.\n",
    "    horizon : int\n",
    "        The horizon to forecast.\n",
    "    base_alg : {ExponentialSmoothing, SimpleExpSmoothing, Holt, ARIMA}\n",
    "        The name of base algoritm for making density forecast.\n",
    "    base_params : dict\n",
    "        A Dictionary with base algorithm parameters.\n",
    "    bins: int or sequence of scalars or str, optional\n",
    "        Define how to calculate bins.\n",
    "    fittedvalues: bool\n",
    "        Include fitted values in density dictionaries or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    if omega is not None:\n",
    "        bins = omega\n",
    "        \n",
    "    if base_alg == ARIMA:\n",
    "        alg = base_alg(ts, exog, **base_params).fit(**fit_params)\n",
    "    else:\n",
    "        alg = base_alg(ts, **base_params).fit(**fit_params)\n",
    "    \n",
    "    if fittedvalues:\n",
    "        alg_preds = alg.predict(start=0, end=len(ts) + horizon - 1)\n",
    "        density_dicts = [{'bins': [], 'probs': [], 'dotted_forecast': None} for _ in range(len(ts) + horizon)]\n",
    "    else:\n",
    "        alg_preds = alg.predict(start=len(ts), end=len(ts) + horizon - 1)\n",
    "        density_dicts = [{'bins': [], 'probs': [], 'dotted_forecast': None} for _ in range(horizon)]\n",
    "    \n",
    "    for i in range(len(alg_preds)):\n",
    "        density_dicts[i]['dotted_forecast'] = alg_preds.iloc[i]\n",
    "        \n",
    "        current_density = alg.resid + alg_preds.iloc[i]\n",
    "        probs, bins = np.histogram(current_density, bins=bins)\n",
    "        density_dicts[i]['probs'], density_dicts[i]['bins'] = probs / np.sum(probs), bins\n",
    "    \n",
    "    return density_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f584a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_forecast(ts, delay, base_alg, base_params={}, fit_params={},\n",
    "                          ax=None, **kwargs):\n",
    "        \n",
    "    density_dict = get_density_forecast(ts, delay, base_alg, base_params, fit_params, **kwargs)[delay - 1]\n",
    "\n",
    "    left_edges = density_dict['bins'][:-1]\n",
    "\n",
    "    colors = []\n",
    "    for i in range(len(left_edges) - 1):\n",
    "        if left_edges[i] < density_dict['dotted_forecast'] < left_edges[i+1]:\n",
    "            colors.append('coral')\n",
    "        else:\n",
    "            colors.append('royalblue')\n",
    "    \n",
    "    alg_name = str(base_alg)[str(base_alg).find('model.') + 6:-2]\n",
    "    \n",
    "    if ax:\n",
    "        ax.bar(left_edges, density_dict['probs'], align='edge',\n",
    "               width=0.9*(left_edges[1] - left_edges[0]), color=colors)\n",
    "        ax.set_title(f'{alg_name}: density forecast with delay={delay}')\n",
    "    else:\n",
    "        plt.bar(left_edges, density_dict['probs'], align='edge',\n",
    "               width=0.9*(left_edges[1] - left_edges[0]), color=colors)\n",
    "        plt.title(f'{alg_name}: density forecast with delay={delay}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b8b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_omega(ts, mode, bins=None, quantile=0.1):\n",
    "    if mode == \"basic\":\n",
    "        return np.histogram_bin_edges(ts, bins)\n",
    "    if mode == \"quantile\":\n",
    "        bin_width = int(np.quantile(abs(ts - ts.shift(1))[1:], quantile))\n",
    "    elif mode == \"mean\":\n",
    "        bin_width = int(np.mean(abs(ts - ts.shift(1))[1:]))\n",
    "    \n",
    "    if bin_width < 1:\n",
    "        bin_width = 1\n",
    "    \n",
    "    min_o = int(np.floor(min(ts.values)))\n",
    "    max_o = int(np.ceil(max(ts.values)))\n",
    "    \n",
    "    bin_edges = [min_o]\n",
    "    while bin_edges[-1] < max_o:\n",
    "        bin_edges.append(bin_edges[-1] + bin_width)\n",
    "    \n",
    "    return np.array(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5898050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_loss(y_true, density_dict, p=2):\n",
    "    \"\"\"\n",
    "    Returns np.array of brier scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : float\n",
    "        A true value.\n",
    "    density_dict : dict\n",
    "        Dict with bins and probabilities information.\n",
    "    \"\"\"\n",
    "    bins_number = density_dict['probs'].size\n",
    "    bins_true = [0] * bins_number\n",
    "\n",
    "    for i in range(bins_number):\n",
    "        if (density_dict['bins'][i] <= y_true <= density_dict['bins'][i + 1]):\n",
    "            bins_true[i] = 1\n",
    "            break\n",
    "    \n",
    "    brier_loss = np.sum((abs(density_dict['probs'] - bins_true))**p)\n",
    "    \n",
    "    if np.sum(bins_true) == 0:\n",
    "        brier_loss += 1\n",
    "        \n",
    "    return brier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8dfcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generalized_loss(y_true, density_dicts, loss_function, p):\n",
    "    \"\"\"\n",
    "    Returns np.array of brier scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : float\n",
    "        A true value.\n",
    "    density_dicts : dict or array-like of dicts\n",
    "        Dicts with bins and probabilities information.\n",
    "    \"\"\"\n",
    "    if type(density_dicts) == dict:\n",
    "        density_dicts = [density_dicts]\n",
    "    \n",
    "    losses = [np.nan] * len(density_dicts)\n",
    "    \n",
    "    for density_dict_count, density_dict in enumerate(density_dicts):\n",
    "        losses[density_dict_count] = loss_function(y_true, density_dict, p)\n",
    "    \n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987b50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_overflowing(base, power_array):\n",
    "    maximum = np.max(power_array)\n",
    "    minimum = np.min(power_array)\n",
    "    \n",
    "    pmax = -np.log(base)/np.log(2) * maximum\n",
    "    pmin = -np.log(base)/np.log(2) * minimum\n",
    "    \n",
    "    if np.abs(pmax-pmin) > 2097:\n",
    "        print('Overflow is imminent. Further calculations are not advised')\n",
    "        return base ** power_array\n",
    "    power_shift = abs((51+pmin+pmax)/2)\n",
    "    power_shift = power_shift + min(0, pmin - power_shift + 1023)\n",
    "    \n",
    "    power_array = power_array - np.abs(power_shift * np.log(2) / np.log(base))\n",
    "    \n",
    "    return base ** power_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c891fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generalized_prediction(ts, preds, omega, weights, loss_function, p, eta):\n",
    "    generalized_predictions = []\n",
    "    \n",
    "    for w in zip(omega, omega[1:]):\n",
    "        losses = get_generalized_loss((w[0] + w[1]) / 2, preds, loss_function, p)\n",
    "        exp_losses = avoid_overflowing(np.e, -eta * losses)\n",
    "        generalized_predictions.append(-(1 / eta) * np.log(np.sum(weights * exp_losses)))\n",
    "        \n",
    "    return np.array(generalized_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33b19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "def s_equation(s, generalized_predictions, m=2):\n",
    "    return np.sum([max(x,0) for x in s - generalized_predictions]) - m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218c9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitution_function(generalized_predictions, s, m=2):\n",
    "    predictions = [max(x,0) / m for x in (s - generalized_predictions)]\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4326a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, losses, eta=1):\n",
    "    exp_losses = avoid_overflowing(np.e, -eta * np.array(losses))\n",
    "    new_weights = weights * exp_losses\n",
    "    return new_weights / (np.sum(new_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4bb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregating_algorithm(ts, horizon, base_alg_list, bins=10, omega_mode=\"basic\",\n",
    "                          loss_function=brier_loss, exog=None, m=2, p=2, weights=None, eta=1):\n",
    "    \"\"\"\n",
    "    Returns density dictionary {'bins': np.array, 'probs': np.array, 'dotted_forecast': float}.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : array_like\n",
    "        The time series to model.\n",
    "    horizon : int\n",
    "        The delay to forecast.\n",
    "    base_alg_list : dict\n",
    "        The dictionary with the names of base algoritms and their params:\n",
    "        base_alg {ExponentialSmoothing, SimpleExpSmoothing, Holt} - name of base algorithm.\n",
    "        base_alg_params : dict - a dictionary of base algorithm's parameters.\n",
    "    loss_function : function\n",
    "        The loss function of aggregating algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    T = len(ts)\n",
    "    K = len(base_alg_list)\n",
    "    \n",
    "    AA_preds = [{} for i in range(T + horizon)]\n",
    "    BA_preds = np.array([{} for i in range((T + horizon) * K)]).reshape(K, T + horizon) \n",
    "    \n",
    "    if not weights:\n",
    "        weights = np.full(K, 1/K)\n",
    "    \n",
    "    omega = get_omega(ts, mode=omega_mode, bins=bins) # здесь в Omega прогнозы не учитываются\n",
    " \n",
    "    i = 0\n",
    "    for base_alg, base_alg_params, fit_alg_params in base_alg_list:\n",
    "        BA_preds[i] = get_density_forecast(ts, horizon, base_alg, base_params=base_alg_params,\n",
    "                                           fit_params=fit_alg_params, omega=omega, exog=exog, fittedvalues=True)\n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "    losses, prev_losses = None, None\n",
    "        \n",
    "    for t in tqdm(range(T + horizon), leave=False):\n",
    "        preds = BA_preds[:, t]\n",
    "        \n",
    "        if not prev_losses:\n",
    "            prev_losses = [loss_function(ts.values[t], pred, p) for pred in preds]  # cheat\n",
    "              \n",
    "        generalized_predictions = get_generalized_prediction(ts.values[:t], preds, omega,\n",
    "                                                             weights, loss_function, p, eta)\n",
    "        \n",
    "        #solving the equation to find s\n",
    "        s_init = np.max(generalized_predictions)\n",
    "        s = fsolve(s_equation, s_init, args=(generalized_predictions, m))\n",
    "        \n",
    "        #get real prediction with substitution function\n",
    "        AA_preds[t]['bins'] = BA_preds[:, t][0]['bins']\n",
    "        real_predictions = substitution_function(generalized_predictions, s, m)\n",
    "        AA_preds[t]['probs'] = real_predictions\n",
    "        \n",
    "        #update weights \n",
    "        if t < T:            \n",
    "            losses = [loss_function(ts.values[t], pred, p) for pred in preds]\n",
    "            weights = update_weights(weights, losses, eta)\n",
    "\n",
    "            prev_losses = losses\n",
    "    \n",
    "    return AA_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0fe6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim_m(ts, base_alg_list, omega_mode=\"basic\",\n",
    "                exog=None, bins=10, p=2):\n",
    "    best_m = 0\n",
    "    best_loss = 1000\n",
    "    AA_losses = {}\n",
    "    # (np.linspace(0.1, 10, 100),\n",
    "    for m in tqdm(np.linspace(0.5, 10, 20), leave=False):\n",
    "        AA_preds = aggregating_algorithm(ts, 0, base_alg_list,\n",
    "                                         bins=bins, omega_mode=omega_mode, exog=exog, m=m, p=p)\n",
    "        AA_losses[m] = []\n",
    "        for i in range(len(ts.values)):\n",
    "            AA_losses[m].append((brier_loss(ts.values[i], AA_preds[i])))\n",
    "        loss = np.mean(AA_losses[m])\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_m = m\n",
    "            best_loss = loss\n",
    "            \n",
    "    n_bins = AA_preds[0]['probs'].size\n",
    "    return best_m, best_loss, n_bins, AA_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76091910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optim_m(all_losses_dict, best_m, ax=plt):\n",
    "    all_losses = [(key, np.mean(values)) for key, values in all_losses_dict.items()]\n",
    "    m_list, loss_list = zip(*all_losses)\n",
    "\n",
    "    ax.plot(m_list, loss_list)\n",
    "    \n",
    "    x, y = best_m, np.mean(all_losses_dict[best_m])\n",
    "    ax.scatter(x, y, s=70, c=\"orange\", zorder=3)\n",
    "    ax.text(x - 0.4, y + 0.02, '({}, {})'.format(x, round(y, 2)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fd425d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_arima(endog, exog, grid_params):\n",
    "    opt_model = {'opt_params': {'order': (0, 0, 0), 'seasonal_order': (0, 0, 0, 0)},\n",
    "                 'opt_bic': 10**6, 'opt_model': None}\n",
    "    \n",
    "    # 'opt_params': {'p': 0, 'd':0, 'q': 0, 'P': 0, 'D':0, 'Q': 0, 's': 0}\n",
    "\n",
    "    grid = ParameterGrid(grid_params)\n",
    "    for params in tqdm(grid, leave=False):\n",
    "        try:\n",
    "            arima = ARIMA(endog,\n",
    "                      exog,\n",
    "                      order=(params['p'], params['d'], params['q']),\n",
    "                      seasonal_order=(params['P'], params['D'], params['Q'], params['s'])).fit()\n",
    "        except:\n",
    "            print(\"LU decomposition error\")\n",
    "            continue\n",
    "\n",
    "        if arima.bic < opt_model['opt_bic']:\n",
    "            opt_model['opt_params']['order'] = (params['p'], params['d'], params['q'])\n",
    "            opt_model['opt_params']['seasonal_order'] = (params['P'], params['D'], params['Q'], params['s'])\n",
    "            opt_model['opt_bic'] = arima.bic\n",
    "            opt_model['opt_model'] = arima\n",
    "    \n",
    "    return opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d06247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auto_base_alg_list(endog, exog, grid_params):\n",
    "    auto_base_alg_list = []\n",
    "    \n",
    "    opt_arima = get_opt_arima(endog, exog, grid_params)\n",
    "    auto_base_alg_list.append((ARIMA, opt_arima['opt_params'], {}))\n",
    "    \n",
    "    SES = ExponentialSmoothing(endog).fit(optimized=True)\n",
    "    auto_base_alg_list.append((ExponentialSmoothing, {},\n",
    "                               {'smoothing_level': SES.params['smoothing_level']}))\n",
    "    \n",
    "    ES_add7 = ExponentialSmoothing(endog, seasonal=\"add\",\n",
    "                                   seasonal_periods=7).fit(optimized=True)\n",
    "    auto_base_alg_list.append((ExponentialSmoothing, {\"seasonal\": \"add\", \"seasonal_periods\": 7},\n",
    "                               {'smoothing_level': ES_add7.params['smoothing_level'],\n",
    "                                'smoothing_seasonal': ES_add7.params['smoothing_seasonal']}))\n",
    "    \n",
    "    return auto_base_alg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "813a429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(ts, horizon, base_alg_list, omega_mode, best_m, ax, bins=10, exog=None,\n",
    "                title='', legend=[], number_to_plot=None):\n",
    "    omega = get_omega(ts, mode=omega_mode, bins=bins)\n",
    "    \n",
    "    pred_dict = {}\n",
    "    i = 1\n",
    "    for base_alg, base_params, fit_params in base_alg_list:\n",
    "        pred_dict[f\"alg{i}\"] = get_density_forecast(ts, horizon, base_alg, base_params, fit_params,\n",
    "                                                    bins=bins, omega=omega, exog=exog, fittedvalues=True)\n",
    "        i += 1\n",
    "    \n",
    "    pred_dict[\"AA\"] = aggregating_algorithm(ts, horizon, base_alg_list, exog=exog,\n",
    "                                            bins=bins, omega_mode=omega_mode, m=best_m)\n",
    "\n",
    "    losses_dict = {key : [] for key in pred_dict.keys()}\n",
    "    \n",
    "    for i in range(len(ts.values)):\n",
    "        for alg in losses_dict.keys():\n",
    "            losses_dict[alg].append(brier_loss(ts.values[i], pred_dict[alg][i]))\n",
    "          \n",
    "    for alg in losses_dict.keys():\n",
    "        losses_dict[alg] = np.cumsum(losses_dict[alg]) / list(range(1, len(ts.values) + 1))\n",
    "        \n",
    "    theoretical_bound = (np.array(list(losses_dict.values())).min(axis=0)\n",
    "                         + np.log(len(losses_dict)) / list(range(1, len(ts.values) + 1)))\n",
    "    \n",
    "    tb_errors = (losses_dict[\"AA\"] > theoretical_bound).sum()\n",
    "    if tb_errors == 0:\n",
    "        print(\"Theoretical bound met\")\n",
    "    else:\n",
    "        print(f\"Theoretical bound not met: {tb_errors / ts.values.size}% violations\")\n",
    "        \n",
    "    losses_dict_sorted = {k: v for k, v in sorted(losses_dict.items(), key=lambda item: item[1].mean())}\n",
    "    \n",
    "    if not number_to_plot:\n",
    "        number_to_plot = len(base_alg_list)\n",
    "    \n",
    "    real_legend = []\n",
    "    i = 0\n",
    "    for alg in losses_dict_sorted.keys():\n",
    "        if i < number_to_plot:\n",
    "            if alg == \"AA\":\n",
    "                continue\n",
    "            ax.plot(ts.index, losses_dict_sorted[alg])\n",
    "            i += 1\n",
    "            real_legend.append(legend[int(alg[-1]) - 1])\n",
    "            print(f\"{legend[int(alg[-1]) - 1]} mean loss: {losses_dict_sorted[alg].mean()}\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    ax.plot(ts.index, losses_dict_sorted['AA'])\n",
    "    print(f\"AA mean loss: {losses_dict_sorted['AA'].mean()}\")\n",
    "    real_legend.append('AA')\n",
    "    ax.plot(ts.index, theoretical_bound)\n",
    "    real_legend.append('TB')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.legend(real_legend, loc='upper right');\n",
    "    return losses_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c8c3f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_predictions(train_dict, test_dict, horizon, base_alg_list, grid_params,\n",
    "                        omega_mode=\"basic\", best_m=\"optim\", is_exog=False, bins=10):\n",
    "    pred_dict = {}\n",
    "    losses_dict = {}\n",
    "    \n",
    "    for ts_name in tqdm(train_dict.keys()):\n",
    "        if is_exog:\n",
    "            ts_train = train_dict[ts_name][\"ts\"]\n",
    "            ts_test = test_dict[ts_name][\"ts\"]\n",
    "            exog_train = (pd.DataFrame(data=[train_dict[ts_name][\"actual_price\"],\n",
    "                                             train_dict[ts_name][\"promo\"]])\n",
    "                          .transpose().astype({\"Actual_Price\": float, \"Promo\": int}))\n",
    "            exog_test = (pd.DataFrame(data=[test_dict[ts_name][\"actual_price\"],\n",
    "                                            test_dict[ts_name][\"promo\"]])\n",
    "                         .transpose().astype({\"Actual_Price\": float, \"Promo\": int}))\n",
    "        else:\n",
    "            ts_train = train_dict[ts_name]\n",
    "            ts_test = tesr_dict[ts_name]\n",
    "            exog_train = None\n",
    "            exog_test = None\n",
    "        \n",
    "        auto_base_alg_list = get_auto_base_alg_list(ts_train, exog_train, grid_params)\n",
    "        all_base_alg_list = base_alg_list + auto_base_alg_list\n",
    "        \n",
    "        if best_m == \"optim\":\n",
    "            best_m, _, _, _ = get_optim_m(ts_train, all_base_alg_list, exog=exog_train,\n",
    "                                          bins=bins, omega_mode=omega_mode)\n",
    "        \n",
    "        omega = get_omega(ts_test, mode=omega_mode, bins=bins)\n",
    "        \n",
    "        pred_dict[ts_name] = {}\n",
    "        i = 1\n",
    "        for base_alg, base_params, fit_params in all_base_alg_list:\n",
    "            pred_dict[ts_name][f\"alg{i}\"] = get_density_forecast(ts_test, horizon, base_alg, base_params, fit_params,\n",
    "                                                                 exog=exog_test, bins=bins,\n",
    "                                                                 omega=omega, fittedvalues=True)\n",
    "            i += 1\n",
    "\n",
    "        pred_dict[ts_name][\"AA\"] = aggregating_algorithm(ts_test, horizon, all_base_alg_list, exog=exog_test,\n",
    "                                                         bins=bins, omega_mode=omega_mode, m=best_m)\n",
    "\n",
    "        losses_dict[ts_name] = {key : [] for key in pred_dict[ts_name].keys()}\n",
    "\n",
    "        for i in range(len(ts_test.values)):\n",
    "            for alg in losses_dict[ts_name].keys():\n",
    "                losses_dict[ts_name][alg].append(brier_loss(ts_test.values[i], pred_dict[ts_name][alg][i]))\n",
    "\n",
    "        for alg in losses_dict[ts_name].keys():\n",
    "            losses_dict[ts_name][alg] = np.cumsum(losses_dict[ts_name][alg]) / list(range(1, len(ts_test.values) + 1))\n",
    "    \n",
    "    return pred_dict, losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "27f60e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_losses(losses_dict, ax, title, legend, number_to_plot=None):\n",
    "    total_losses = {alg: np.zeros_like(losses) for alg, losses in losses_dict[next(iter(losses_dict))].items()}\n",
    "            \n",
    "    for ts_losses in losses_dict.values():\n",
    "        for alg, losses in ts_losses.items():\n",
    "            total_losses[alg] += losses\n",
    "    \n",
    "    for alg, losses in total_losses.items():\n",
    "            total_losses[alg] = losses / len(losses_dict)\n",
    "            \n",
    "    total_losses_sorted = {k: v for k, v in sorted(total_losses.items(), key=lambda item: item[1].mean())}\n",
    "            \n",
    "    if not number_to_plot:\n",
    "        number_to_plot = len(legend) - 2\n",
    "    \n",
    "    real_legend = []\n",
    "    i = 0\n",
    "    for alg in total_losses_sorted.keys():\n",
    "        if i < number_to_plot:\n",
    "            if alg == \"AA\":\n",
    "                continue\n",
    "            ax.plot(total_losses_sorted[alg])\n",
    "            i += 1\n",
    "            real_legend.append(legend[int(alg[-1]) - 1])\n",
    "            print(f\"{legend[int(alg[-1]) - 1]} mean loss: {total_losses_sorted[alg].mean()}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    ax.plot(total_losses_sorted['AA'])\n",
    "    print(f\"AA mean loss: {total_losses_sorted['AA'].mean()}\")\n",
    "    real_legend.append('AA')\n",
    "        \n",
    "    ax.set_title(title)\n",
    "    ax.legend(real_legend, loc='lower right');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
